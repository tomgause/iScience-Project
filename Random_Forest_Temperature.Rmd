---
title: "Random Forest Temperature"
author: "Acadia Hegedus"
date: "3/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Read in relevant libraries. 
```{r}
library(data.table)
library(rvest)
library(stringr)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(readxl)
library(class)
library(FNN)
library(MASS)
library(ISLR)
library(rpart)
library(rattle)
library(ipred)
library(randomForest)
library(caret)
library(spotifyr)
library(dendextend)
```

Load in our data set. 
```{r}
hindcast_data <- readRDS(file.choose()) #look at random sample of data
hindcast_data_subset <- hindcast_data
hindcast_data_subset <- hindcast_data_subset %>%
  mutate(unique_row_identifer = rownames(hindcast_data_subset))
rownames(hindcast_data_subset) <- seq(1,540,1)
```

Create our target variable: temperature bias
A positive bias indicates the forecast underpredicted.
A negative bias indicates the forecast overpredicted.
```{r}
hindcast_data_subset <- hindcast_data_subset %>%
  mutate(bias_tmp = obs_tmp_k-fcst_tmp_k)
```


Variables to use are: 
- forecast_target
- x
- y
- target_month
- lead
- fcst_tmp_k
- bias_tmp ()

Divide into training and testing data set. Total (sampled) data is 540 data points. 

```{r}
training.data <- hindcast_data_subset[sample(nrow(hindcast_data_subset), 500, replace = FALSE),]
testing.data <- hindcast_data_subset[-as.numeric(rownames(training.data)),]
```

Run a PCA, to explore relationship between variables. 
```{r}
data_for_pca <- hindcast_data_subset%>%
  dplyr::select(-unique_row_identifer)

pca1 <- prcomp(data_for_pca, scale = TRUE)
biplot(pca1)
```



Take out correlated variables. There don't seem to be any. 

Use k-fold cross validation to minimize error for a random forest. Can use all variables, both quantitative and qualitative. 

To optimize our random forest, we will need to pick the best hyperparameters including mtry (the number of variables for each bagged regression tree to consider), node size (the minimum size of a terminal node), and bootstrap resample size. We fix ntree (the number of trees in our random forest) to be 100 and optimize it later, as we know more trees in our forest will only increase the model's performance. We define error here as the out-of-bag mean squared error of the random forest's predictions.


```{r}
 metric.data <- data.frame(0,0,0,0)
 colnames(metric.data) <- c("i", "j", "k", "error")
 
 for (i in 1:5){ #range of mtry 
   for (j in 1:10){ #range of nodesizes ADAPT THIS RANGE?
     for (k in seq(100,500, by = 50)){ #range of bootstrap resample sizes
       rf <- randomForest(bias_tmp ~ target_month + x + y + lead + fcst_tmp_k,
                   data = training.data, mtry = i, nodesize = j,sampsize = k, ntree = 100) #this is lower than we would like, but we will change later
 
       error <- tail(rf$mse, 1) #this came from https://stats.stackexchange.com/questions/369134/random-forest-out-of-bag-rmse
       #rf$mse gives OOB mse for bagging 1:n trees, so last mse in this vector gives the OOB mse of the entire forest
   
       newdf <- data.frame(i,j,k,error)
       metric.data <- rbind(metric.data, newdf)
     }
   }
   print(i)
 }
 
 colnames(metric.data) <- c("mtry","nodesize","resamplesize",
                            "error")

```

We save this run as a csv so we don't have to rerun this lengthy optimization when we knit.

```{r}
metric.data1 <- metric.data
metric.data1 <- metric.data1[2:451,] #remove 0,0,0,0 row
which.min.error <- which.min(metric.data1$error)
best.parameters <- metric.data1[which.min.error,]
print(paste("The most ideal hyperparameters are mtry = ", best.parameters$mtry, ", nodesize = ", best.parameters$nodesize, ", bootstrap resample size = ", best.parameters$resamplesize, ", with an error of", round(best.parameters$error)))
```

To optimize the random forest hyperparameters, all values of mtry were tested (1:13), where 13 is the number of possible input variables. For node size, only 1:10 were tested, and the 5 smallest error random forests had node sizes far below from the maximum tested value of 10. So, I decide not to test higher minimum node sizes. For the bootstrap resample size, I chose to test a range of values up to the maximum number of rows in the data set. I assumed a resample smaller than ~50% of my data set would not be effective. Ideally, I would test more granular values for this bootstrap resample size, but in the interest of time, I leave it at this. 

Now that we have our ideal mtry, node size, and resample size, all that's left is to choose ntree, or the number of trees. As noted earlier, more trees means less error, so let's see what a reasonable optimal ntree might be, considering run time. Let's compare the error from using ntree = 100 vs. ntree = 500.

```{r}
#try ntree = 100 (what was done above)
#we know error = 127, as was calculated above
system.time(rf100 <- randomForest(ActivePower ~ .,
                 data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 100))
```

Ntree = 100 took just over a minute to run, with an error of 127.

```{r}
#try ntree = 500
 rf500 <- randomForest(ActivePower ~ .,
                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 500)
 
 system.time(rf500 <- randomForest(ActivePower ~ .,
                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 500))
 
 tail(rf500$mse,1)
```

Running our random forest with 500 trees caused R to crash, so we are going to assume that 500 trees with our data set is too much for our computer to handle. Ideally we would have more than 100 trees in our forest, but we will have to stick with 100 trees to respect our computer's capabilities.







