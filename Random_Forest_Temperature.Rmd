---
title: "Random Forest Temperature"
author: "Acadia Hegedus, Katelyn Mei, and Tom Gaus"
date: "4/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

Read in relevant libraries. 
```{r}
library(data.table)
library(rvest)
library(stringr)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(readxl)
library(class)
library(FNN)
library(MASS)
library(ISLR)
library(rpart)
library(rattle)
library(ipred)
library(randomForest)
library(caret)
library(dendextend)
library(ranger)
```

Load in our data sets. 
```{r}
training.data <- readRDS("./data/train_subset__2022-04-06_01-10-09.RDS")
testing.data <- readRDS("./data/test_subset_2022-04-06_01-19-58.RDS")

training.data <- na.omit(training.data)

testing.data <- na.omit(testing.data)

training.data <- training.data%>%
  dplyr::select(-obs_cell)

testing.data2 <- testing.data%>%
  dplyr::select(-fcst_qm_pr_m_day,-fcst_qm_tmp_k)

all.data <- rbind(training.data, testing.data2)
```


Predictor variables to use are: 
- x
- y
- forecast_target
- target_month
- lead
- fcst_tmp_k
- elevation 
- c(lag1--> lag12) 
- fcst_pr_m_day 
***fcst_pr_m_day: forecast precipitation ***
***lag1 : observed temperature one month ago***

Target variable:
- obs_tmp_k

Run a PCA, to explore relationship between variables. 
```{r}
data.for.pca <- all.data%>%
  dplyr::select(-fcst_cell,-obs_tmp_k.x,-obs_pr_m_day.x, -forecast_timestamp, -forecast_target)

pca1 <- prcomp(data.for.pca, scale = TRUE)

pca1 #see outputs of PCA

plot(pca1) #see how much variance is explained by first few PCs

plot(cumsum(pca1$sdev^2/4))

```

After running PCA only with variables we may use as predictor variables in our model (removing obs_cell, fcst_cell, obs_tmp_k, obs_pr_m_day), along with removing  the non-numeric variables (forecast_timestamp and forecast_target), no coefficients of PC1 are exactly the same. So, it looks like no variables are so highly correlated that they need to be removed. 

It looks like the first 3-5 PCs can explain the majority of the variance in the data. Let's try training a random forest with the first 3 principal components and compare the run time to using 5, and all 20. First, we must generate PCs on the training data only. 

```{r}
data.for.pca <- training.data%>%
  dplyr::select(-fcst_cell,-obs_tmp_k.x,-obs_pr_m_day.x, -forecast_timestamp, -forecast_target)

pca2 <- prcomp(data.for.pca, scale = TRUE)

pca2 #see outputs of PCA

plot(pca2) #see how much variance is explained by first few PCs

plot(cumsum(pca2$sdev^2/4))
```


```{r}
pcs <- as.data.frame(pca2$x)
```


Train with PC1-PC3. Elapsed time: 160 seconds
```{r}
pcs3 <- pcs%>%
  dplyr::select(PC1, PC2, PC3)%>%
  mutate(obs_tmp_k = training.data$obs_tmp_k.x)

system.time(rf.pcs3 <- ranger(obs_tmp_k ~ ., 
               data = pcs3,
               num.trees = 10,
               mtry = 3, 
               min.node.size = 1000,
               sample.fraction = 0.8,
               num.threads = 16))
```

Train with PC1-PC5. Elapsed time: 172 seconds.
```{r}
pcs5 <- pcs%>%
  dplyr::select(PC1, PC2, PC3, PC4, PC5)%>%
  mutate(obs_tmp_k = training.data$obs_tmp_k.x)

system.time(rf.pcs5 <- ranger(obs_tmp_k ~ ., 
               data = pcs5,
               num.trees = 10,
               mtry = 3, 
               min.node.size = 1000,
               sample.fraction = 0.8,
               num.threads = 16))
```

Train with all 20 variables. Elapsed time: 122 seconds.
```{r}
system.time(rf.20 <- ranger(obs_tmp_k.x ~ x + y + forecast_target + target_month + lead + fcst_tmp_k + elevation + lag1 + lag2 + lag3 + lag4 + lag5 + lag6 + lag7 + lag8 + lag9 + lag10 + lag11 + lag12 + fcst_pr_m_day, 
               data = training.data,
               num.trees = 10,
               mtry = 3, 
               min.node.size = 1000,
               sample.fraction = 0.8,
               num.threads = 16))
```

To train our random forest, we will use all variables, both quantitative and qualitative, except for forecast_timestamp. We exclude forecast_timestamp as it is embedded in lead and forecast_target. This leaves us with the following 20 predictor variables: x, y, forecast_target, target_month, lead, fcst_tmp_k, elevation, lag1, lag2, lag3, lag4, lag5, lag6, lag7, lag8, lag9, lag10, lag11, lag12, fcst_pr_m_day.  

To optimize our random forest, we will need to pick the best hyperparameters including mtry (the number of variables for each bagged regression tree to consider), node size (the minimum size of a terminal node), and bootstrap resample size. We fix ntree (the number of trees in our random forest) to be 100 and optimize it later, as we know more trees in our forest will only increase the model's performance. We define error here as the out-of-bag mean squared error of the random forest's predictions.


```{r}
 metric.data <- data.frame(0,0,0,0)
 colnames(metric.data) <- c("i", "j", "k", "error")
 
 for (i in 1:20){ #range of mtry 
   for (j in c(10,100,1000,10000,100000,1000000)){ #range of nodesizes
     for (k in seq(nrow(hindcast_data)/2,nrow(hindcast_data), by = 10000000)){ #range of bootstrap resample sizes (around 49 options)
       rf <- randomForest(bias_tmp ~ x + y + forecast_target + target_month + lead + fcst_tmp_k + elevation + lag1 + lag2 + lag3 + lag4 + lag5 + lag6 + lag7 + lag8 + lag9 + lag10 + lag11 + lag12 + fcst_pr_m_day,
                   data = training.data, mtry = i, nodesize = j,sampsize = k, ntree = 100) #this is lower than we would like, but we will change later
 
       error <- tail(rf$mse, 1) #this came from https://stats.stackexchange.com/questions/369134/random-forest-out-of-bag-rmse
       #rf$mse gives OOB mse for bagging 1:n trees, so last mse in this vector gives the OOB mse of the entire forest
   
       newdf <- data.frame(i,j,k,error)
       metric.data <- rbind(metric.data, newdf)
     }
   }
   print(i)
 }
 
 colnames(metric.data) <- c("mtry","nodesize","resamplesize",
                            "error")

```

We save this run as a csv so we don't have to rerun this lengthy optimization when we knit.

```{r}
write.csv(metric.data, "Random_Forest_Temperature_MetricData.csv")
```


Print out optimal hyperparameters.

```{r}
metric.data1 <- metric.data
metric.data1 <- metric.data1[2:nrow(metric.data1),] #remove 0,0,0,0 row
which.min.error <- which.min(metric.data1$error)
best.parameters <- metric.data1[which.min.error,]
print(paste("The most ideal hyperparameters are mtry = ", best.parameters$mtry, ", nodesize = ", best.parameters$nodesize, ", bootstrap resample size = ", best.parameters$resamplesize, ", with an error of", round(best.parameters$error)))
```

To optimize the random forest hyperparameters, all values of mtry were tested (1:20), where 20 is the number of possible input variables. For node size, a logarithmic scale sequence was tested. For the bootstrap resample size, we chose to test a range of values up to the maximum number of rows in the data set. I assumed a resample smaller than ~50% of my data set would not be effective. 

Next step: look at output of metric.data, see if we need to test more granlar bootstrap resample size, nodesize.



_________________________________________________________________________________
Now that we have our ideal mtry, node size, and resample size, all that's left is to choose ntree, or the number of trees. As noted earlier, more trees means less error, so let's see what a reasonable optimal ntree might be, considering run time. Let's compare the error from using ntree = 100 vs. ntree = 500.

```{r}
#try ntree = 100 (what was done above)
#we know error = X, as was calculated above
# system.time(rf100 <- randomForest(ActivePower ~ .,
#                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 100))
```

Ntree = 100 took X minutes to run, with an error of X.

```{r}
#try ntree = 500
 # rf500 <- randomForest(ActivePower ~ .,
 #                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 500)
 # 
 # system.time(rf500 <- randomForest(ActivePower ~ .,
 #                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 500))
 
 # tail(rf500$mse,1)
```

Ntree = 500 took X minutes to run, with an error of X.







